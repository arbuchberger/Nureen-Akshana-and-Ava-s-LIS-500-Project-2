<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <title>Dr. Joy Buolamwini on Machine Learning</title>
    <style>
        body {
            font-family: system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial, sans-serif;
            line-height: 1.6;
            margin: 2rem;
            color: #1f2937;
        }

        main {
            max-width: 800px;
            margin: auto;
        }

        h1 {
            font-size: 1.8rem;
            margin-bottom: .25rem;
        }

        h2 {
            font-size: 1.2rem;
            color: #4b5563;
            margin-top: 0;
        }

        p {
            margin: 1rem 0;
        }

        blockquote {
            margin: 1rem 0;
            padding: .75rem 1rem;
            background: #f9fafb;
            border-left: 4px solid #e5e7eb;
        }

        footer {
            margin-top: 2rem;
            font-size: .9rem;
            color: #6b7280;
        }
    </style>
</head>

<body>
    <main>
        <header>
            <h1>Dr. Joy Buolamwini on Machine Learning</h1>
            <h2>Algorithmic Justice, Accountability, and Inclusive AI</h2>
        </header>

        <article>
            <p>
                Dr. Joy Buolamwini—computer scientist and founder of the Algorithmic Justice League—argues that machine
                learning
                systems do not exist in a vacuum: they reflect the data, design choices, and power structures that
                create them.
                Her point of view emphasizes that “accuracy” alone is not enough; models can be highly accurate on the
                groups
                they see most and still fail—and harm—those they underrepresent. Drawing from her research on facial
                analysis,
                she shows how skewed training data can lead to dramatically higher error rates for darker-skinned women
                compared
                with lighter-skinned men, underscoring that bias is a measurable technical risk and a civil-rights
                issue.
            </p>

            <p>
                Buolamwini’s prescription is a lifecycle approach to accountability: audit models before and after
                deployment,
                document datasets and training decisions, measure performance across demographic slices, and give
                impacted
                communities a seat at the table. She advocates for transparency (e.g., model and data sheets),
                meaningful
                contestability when systems make mistakes, and regulatory guardrails that match the stakes of real-world
                use.
                In short, her POV reframes ML progress as not just making models bigger or faster, but making them
                <em>more just</em>—through inclusive data, rigorous evaluation, clear disclosure, and consequences when
                systems
                cause harm.
            </p>

            <blockquote>
                Machine learning isn’t neutral by default; it inherits our choices. If we want fair outcomes, we have to
                design
                for them.
            </blockquote>
        </article>

        <footer>
            <p>
                Note: This page summarizes widely reported themes from Dr. Buolamwini’s public work (e.g., Gender
                Shades,
                Algorithmic Justice League, the “Coded Bias” documentary).
            </p>
        </footer>
    </main>
</body>

</html>